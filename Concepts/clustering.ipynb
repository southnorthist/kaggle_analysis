{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Methods\n",
    "\n",
    "\n",
    "_Summarized by QH_  \n",
    "_First version: 2022-11-02_  \n",
    "_Last updated on : 2022-11-02_  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective for clustering is to find homogeneous subgroups among the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "K-Means clustering method can partition the datasets into $K$ distinct and non-overlapping segments/clusters. The idea of K-Means is to find a good way to partition the data that can make the _within-cluster variation_ as small as possible.\n",
    "\n",
    "To define the _within-cluster variation_, the common way is to use Euclidean distance.\n",
    "* Eclidean distance: $\\sqrt{\\sum_{j=1}^p (x_{ij} - x_{i'j})^2}$ for observation $i$ and $i'$.\n",
    "\n",
    "And the _within-cluster variation_ for cluster $k$ is defined as the sum of Pair-wise squared Euclidean distance: \n",
    "$$ W(C_k) = \\frac{1}{|C_k|} \\sum_{i, i' \\in C_k} \\sum_{j=1}^p (x_{ij} - x_{i'j})^2$$  \n",
    "\n",
    "where $|C_k|$ denotes the number of observations in the $k$ th cluster.\n",
    "\n",
    "Our objective is to minimize for the total $K$ clusters' _within-cluster variation_, mathematically:\n",
    "$$ \\text{min}_{c_1, \\cdots, c_k} \\sum_{k=1}^KW(C_k) $$ \n",
    "\n",
    "\n",
    "\n",
    "The algorithm for a dataset with number of observations being $n$ and number of features being $p$:\n",
    "1. Randomly initial $K$ _centroid_.\n",
    "2. Iterate until the cluster assignments stop changing:  \n",
    "    a. Assign each observation to the closest cluster centroid. Closest is measured using Euclidean distance.  \n",
    "    b. For each of the $K$ clusters, compute the cluster _centroid_. The $k$th cluster centroid is the vector of $p$ features, for each feature value being the average of the observations' feature value in that cluster.\n",
    "\n",
    "Notes:\n",
    "* The algorithm gurantees to decrease the _within-cluster variation_ since each steps is to find the observation that's closest to each other. And mathematically it is because:\n",
    "$$ \\frac{1}{|C_k|} \\sum_{i, i' \\in C_k} \\sum_{j=1}^p (x_{ij} - x_{i'j})^2 = 2\\sum_{i \\in C_k} \\sum_{j=1}^p (x_{ij} - \\bar{x}_{kj})^2$$ \n",
    "* It may get into local optimum given different initialization.\n",
    "\n",
    "Advantages:\n",
    "* Easy and straightforward to implement and understand\n",
    "\n",
    "Drawbacks:\n",
    "* Need to pre-specify number of clusters $K$. \n",
    "    * We can use __elbow method__ using sum of _within-cluster variation_ or _average silhoutte score_ to determine the $K$.\n",
    "    * Use business knowledge together with mathematical methods to determine.\n",
    "* Need to initialize the $K$ centroids which may results in different local optimum. \n",
    "    * Suggest to run multiple times with different initialization and choose the one that minimize the sum of _within-cluster variation_.\n",
    "\n",
    "### Elbow method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Hierarchical Clustering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "1. Introduction to Statistical Learning\n",
    "2. Machine Learning Course by Andrew Ng on Coursera\n",
    "3. Scikit-Learn Online Clustering documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96578ef0327f3af430031f765abc91a1dd1caaa16fb50e1f5b330f8f1788b2ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

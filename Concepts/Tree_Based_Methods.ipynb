{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Based Methods\n",
    "\n",
    "*First version on: 2020-11-11  \n",
    " Latest updates on: 2022-12-03*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prediction/Decision Trees\n",
    "What inspires the decision tree methodology?    \n",
    "\n",
    "Predictive models like linear/polynomial regressions are **global models** which means the formula applies to the entire data space. When there are non-linear and complicated interatice effects among features, it is very hard to construct a **global** solution to do prediction.  \n",
    "\n",
    "Some non-parametric smoothers try to fit the data **locally** and then integrate them. While for Prediction/Decision Trees, **sub-division** or **partition** is used to cut the data space into smaller manageable area (i.e. can be modeled using simple average or taking the majority of votes).\n",
    "\n",
    "Note: This information is adapted from reference [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regression Trees\n",
    "### 2.1 What's the prediction mechanism for regression trees?\n",
    "There are two steps for the training data:  \n",
    "1. Divide the training predictor space (i.e. $X_1, X_2, \\cdots, X_p$) into $J$ distinct and non-overlapping regions, $R_1, R_2, \\cdots, R_J$.\n",
    "2. For every observation that falls into the region $R_j$, we use the same value for its prediction. The prediction value is the mean of the response values of all the training observation in $R_j$.   \n",
    "\n",
    "How do we construct the regions then?  \n",
    "Let's define the objective function using residual sum of squares (i.e. *RSS*):\n",
    "$$ RSS = \\sum_{j=1}^{J}\\sum_{i \\in R_j} \\left( y_i - \\hat{y}_{R_j} \\right)^{2}$$\n",
    "where $\\hat{y}_{R_j}$ is the mean response for the training observations within the $j^{th}$ region $R_j$.  \n",
    "However, it is computationally infeasible to consider every possible partition of the feature space. So we take a *top-down, greedy* approach which is called as *recursive binary splitting*.\n",
    "> *top-down*: The approach starts with the root of the tree   \n",
    "\n",
    "> *greedy*: We are trying to find the best split at each step rather than looking ahead and picking a split that will lead to a better tree in the future step.   \n",
    "\n",
    "The *recursive binary splitting* algorithm:\n",
    "1. Select the predictor $X_j$ and cutpoint $s$ such that splitting the predictor space into the regions $R_1(j, s) = \\{X|X_j<s\\}$ and $R_2(j, s) = \\{X|X_j\\ge s\\}$ that leads to the greatest RSS reduction. i.e., select the value of $j$ and $s$ that minimizes:\n",
    "$$ \\sum_{i: x_i \\in R_1((j, s)} \\left( y_i - \\hat{y}_{R_1}\\right) ^2 + \\sum_{i: x_i \\in R_2((j, s)} \\left( y_i - \\hat{y}_{R_2}\\right) ^2$$\n",
    "2. We repeat the process to find the best predictor and best cutpoint that split the above two regions to minimize the RSS within either of the regions. Note that, we will end up split one of the region and the total regions become three.\n",
    "3. We continue the splitting until no region contains more than $M$ number of observations. Typicall, $M = 5$.\n",
    "\n",
    "### 2.2 Why and how do we prune the trees?\n",
    "The *recursive binary splitting* may end up with very good prediction that there is very small RSS (i.e. small bias). In this case, however, the tree may introduce too much complexity to overfit the data and perform very bad on the training data.    \n",
    "\n",
    "This leads to the process of *tree pruning* which tries to the get best *sub-tree* according to some selected hyper parameter $\\alpha$. The methodology is called *cost complexity pruning* or *weakest link pruning*.   \n",
    "\n",
    "For each value of $\\alpha (>0)$ and a full tree $T$, we can find a sub-tree that minimizes the following objective function:\n",
    "$$\\sum_{m = 1}^{|T|}\\sum_{i: x_i \\in R_m} \\left( y_i -\\hat{y}_{R_m}\\right)^2 + \\alpha|T| $$\n",
    "where $|T|$ is the total number of terminal nodes of tree $T$ and $R_m$ is the area corresponding to the $m^{th}$ terminal node.  \n",
    "\n",
    "Here is the complete Regression Tree Building Algorithm:\n",
    "1. Use *recursive binary splitting* to grow a large tree on the trainng data, stopping only when each terminal note has fewer than the minimum number ($M$) of observations.\n",
    "2. Apply *cost complexity pruning* to the large tree in order to obtain a sequence of best subtress, as a function of $\\alpha$.\n",
    "3. Use *K-fold cross-validation* to choose $\\alpha$.\n",
    "4. Return the subtree from Step 2 that corresponds to the chosen value of $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification Trees\n",
    "Similar to Regression Trees except that the prediction is for a qualitative value: \n",
    "1. Divide the training predictor space (i.e. $X_1, X_2, \\cdots, X_p$) into $J$ distinct and non-overlapping regions, $R_1, R_2, \\cdots, R_J$.\n",
    "2. For every observation that falls into the region $R_j$, we use the same value for its prediction. The prediction value is the _most commonly occuring class_ of all the training observation in $R_j$.   \n",
    "\n",
    "How do we construct the regions then?  \n",
    "We can still use the _recursive binary splitting_ but obviously we need to change the objective function. And a natural one is the _classification error rate_: the fraction of the training observations in that region that do not belong to the most common class:\n",
    "$$ \\text{Error rate} = 1 - \\max_k(\\hat{p}_{mk})$$\n",
    "where $\\hat{p}_{mk}$ is the proportion of training observations in the $m$ th region that are from the $k$ th class.  \n",
    "\n",
    "However, the classification error is not sufficiently sensitive for tree-growing, so in practice, there are two ways to define the objective function:\n",
    "1. _Gini Index_: It measures the total variance across the $K$ classes.\n",
    "$$G = \\sum_{k=1} ^{K}\\hat{p}_{mk} (1 - \\hat{p}_{mk}) $$\n",
    "As can be seen, when $\\hat{p}_{mk}$'s are close to 0 or 1, _Gini Index_ takes on small values. When $\\hat{p}_{mk}$'s are close to 0 or 1 means that a node contains predominantly observations from a single class. And this is why _Gini Index_ is referred as a measure of _purity_ of a node.\n",
    "\n",
    "2. _Entropy_: \n",
    "$$D = -\\sum_{k=1} ^{K}\\hat{p}_{mk} \\log\\hat{p}_{mk}$$\n",
    "One can show that the entropy will take on a value near 0 if the $\\hat{p}_{mk}$’s are all near 0 or near 1. Therefore, like the Gini index, the entropy will take on a small\n",
    "value if the mth node is pure.\n",
    "\n",
    "When we are trying to prune the tree, we can use similar procedures as the regression tree and any of the above three approaches might be used. But the classification error rate is preferable if prediction accuracy of the final pruned tree is the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advantages and Disadvantage of Decision Trees\n",
    "* Advantages:\n",
    "    1. Trees are very easy to explain to people or even a non-expert.\n",
    "    2. Some people believe that decision trees more closely mirror human decision-making.\n",
    "    3. Trees can easily handle qualitative predictors without the need to create dummy variables.\n",
    "* Disadvantages:\n",
    "    1. Trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches\n",
    "    2. Trees can be very non-robust. In other words, a small change in the data can cause a large change in the final estimated tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ensemble Methods based on Trees\n",
    "\n",
    "An _ensemble_ method is an approach that combines many simple 'building block' models in order to obtain a single and potentially very powerful model. We will explore the popular _ensemble_ methods: _bagging_, _random forests_ and _boosting_ which can potentially reduce the disadvantages from a single decision tree model.\n",
    "\n",
    "### 5.1 Bagging\n",
    "_Boostrap aggregation_ or _bagging_: A general-purpose procedure for reducing the variance of a statistical learning mehtods. The principle is that _averaging a set of observations reduces variance_ since $$\\text{var}(\\bar{Z}) = \\frac{\\sigma^2}{n}, Z_i \\overset{i.i.d}{\\sim} N(\\mu, \\sigma)$$\n",
    "\n",
    "In terms of the building block models, we could calculate $\\hat{f}^1(x), \\cdots, \\hat{f}^B(x)$ using $B$ boostrapped training sets, and average them in order to obtain a single low-variance statistical learning model:\n",
    "$$\\hat{f}_{\\text{avg}}(x) = \\frac{1}{B} \\sum_{b=1}^B \\hat{f}^b(x)$$\n",
    "\n",
    "If we combine bagging together with decision tree: growing $B$ unpruned trees and averaging these tree results, it will reduce the variance and improve in accuracy.\n",
    "Specifically, averaging the tree results means:\n",
    "* for regression trees, take the average\n",
    "* for classification tress, take the majority vote (most commonly occuring class among the B predictions).\n",
    "\n",
    "#### 5.1.1 Out-of-Bag Error Estimation\n",
    "We can estimate the test error of a bagged model without the need to perform cross-validation or validation set approach.  \n",
    "One can show that on average, each bagged tree makes use of around two-thirds of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as the _out-of-bag(OOB)_ observations. We can predict each of the OOB observation using the bagged tree and then take the average or majority vote for that observation from all the predictions as the OOB prediction for that observation. This way, we can calculate overall OOB MSE for regression problems or classification error for a classification problem.\n",
    "#### 5.1.2 Variable Importance Measures\n",
    "#### 5.1.3 Development of Bagging Methods\n",
    "* When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting [B1999].\n",
    "* When samples are drawn with replacement, then the method is known as Bagging [B1996].\n",
    "* When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces [H1998].\n",
    "* When base estimators are built on subsets of both samples and features, then the method is known as Random Patches [LG2012].\n",
    "\n",
    "### 5.2 Random Forest\n",
    "_Random Forest_ provide an improvement over bagged trees by way of a small tweak that _decorrelates_ the trees: When building these decision trees, a _random sample of $m$ predictors_ is chosen from as split candidates from the full set of $p$ predictors. We typically choose $m \\approx \\sqrt{p}$, i.e. not allowed to consider a majority of the available predictors.  \n",
    "\n",
    "The reason is very simple: if there is a very strong predictor, highly likely each bagged tree will pick this predictor up as the top split. As a result, all the bagged trees will look similar and highly correlate with each other. And this will not reduce a lot of variance. So random forest simply does not allow strong predictors to dominate the bagged tree splits and decorrelates each bagged trees.\n",
    "\n",
    "So the main difference between _bagging_ and _random forest_ is $m$:\n",
    "* when $m = p$, it is bagging\n",
    "* when $m = \\sqrt{p}$ or other value that is smaller than $p$\n",
    "\n",
    "Some notes:\n",
    "* Using a small value of m in building a random forest will typically be helpful when we have a large number of correlated predictors.\n",
    "* As with bagging, random forests will not overfit if we increase $B$, so in practice we use a value of $B$ sufficiently large for the error rate to have settled down.\n",
    "\n",
    "### 5.3 Boosting\n",
    "Boosting is another way to improve the predictions from a decision tree: Trees are grown _sequentially_: each tree is grown using information from previously grown trees.\n",
    "\n",
    "The algorithm is as follows:\n",
    "1. Set $\\hat{f}^1(x)$ and $r_i = y_i$ for all $i$ in the training set.\n",
    "2. For $b = 1, 2, \\cdots, B$, repeat:\n",
    "    * Fit a tree $\\hat{f}^b$ with $d$ splits ($d + 1$ terminal nodes) to the training data $(X, r)$\n",
    "    * Update $\\hat{f}$ by adding in a shrunken version of the new tree:\n",
    "    $$\\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^b(x)$$\n",
    "    * Update the residuals:\n",
    "    $$r_i \\leftarrow r_i - \\lambda \\hat{f}^b(x)$$\n",
    "3. Output the boosted model:\n",
    "$$\\hat{f}(x) = \\sum_{b = 1}^B \\lambda \\hat{f}^b(x)$$\n",
    "\n",
    "There are three tuning parameters:\n",
    "1. The number of trees $B$. Unlike bagging and random forests, boosting can overfit if B is too large, although this overfitting tends to occur slowly if at all.\n",
    "2. The shrinkage parameter $\\lambda$, a small positive number. This controls the rate at which boosting learns. Typical values are 0.01 or 0.001. Very small $\\lambda$ can require using a very large value of B in order to achieve good performance.\n",
    "3. The number $d$ of splits in each tree, which controls the complexity of the boosted ensemble. Often $d = 1$ works well, in which case each tree is a _stump_, consisting of a single split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1. Gradient Boosting Machines\n",
    "The generalization of AdaBoost is Gradient Boosting which allows arbitrary differentiable loss functions to be used. Rather than fit trees to _residuals_, gradient boosting fit trees to _negative gradients_ of any differentiable loss function:\n",
    "\n",
    "The general steps for both regression and classification are as follows:\n",
    "* Given Data $\\{x_i, y_i\\}_{i = 1}^n$ and a differentiable loss function $L$.\n",
    "* Step 1: Start with an initial model with a constant value: $F_0(x) = \\underset{\\gamma}{\\argmin} \\sum_{i = 1} ^n L(y_i, \\gamma)$\n",
    "* Step 2: For $b = 1, 2, \\cdots, B$:\n",
    "    * Calculate negative gradients or pseudo residual: $ r_{ib} = −g(x_i) = −\\frac{\\partial L(y_i,F(x_i))}{\\partial F(x_i)}$ for $i = 1, 2, \\cdots, n$.\n",
    "    * Fit a regression tree to negative gradients $r_{ib} = −g(x_i)$ and create terminal regions $R_{jb}$, for $j = 1, 2, \\cdots, J_b$.\n",
    "    * For $j = 1, 2, \\cdots, J_b$: compute $\\gamma_{jb} = \\underset{\\gamma}{\\argmin} \\sum_{x_i \\in R_{ij}} L(y_i, F_{b-1}(x_i) + \\gamma)$.\n",
    "    * $F_b(x) := F_{b-1}(x) + \\rho \\sum_{j = 1}^{J_b} \\gamma_{jb} \\cdot I(x \\in R_{jb})$, where $\\rho$ is the learning rate.\n",
    "\n",
    "For regressions:\n",
    "1. We usually use squared loss: $\\frac{1}{2}(y_i - \\hat{y_i})^2$. It is equivalent to fit to _residuals_ since the negative gradient:\n",
    "$$−g(x_i) =  −\\frac{\\partial \\frac{1}{2}(y_i - \\hat{y_i})^2}{\\partial \\hat{y_i}} = y_i - \\hat{y_i} = \\text{residual}$$\n",
    "\n",
    "For Binary classifications:\n",
    "1. We usually use negative log-likelihood (log-loss): $-(y_i \\times \\log \\hat{p_i} + (1 - y_i) \\times \\log (1 - \\hat{p_i}))$. And $F(x) = \\log \\text{odds} (x)$. We can re-write the loss function in terms of log-odds.\n",
    "$$\\begin{aligned}\n",
    "-(y_i \\times \\log \\hat{p_i} + (1 - y_i) \\times \\log (1 - \\hat{p_i})) & = -y_i \\times \\log \\hat{p_i} - (1 - y_i) \\times \\log (1 - \\hat{p_i}) \\\\\n",
    "&= -y_i \\times (\\log \\hat{p_i} - \\log (1- \\hat{p_i})) - \\log (1 - \\hat{p_i}) \\\\\n",
    "&= -y_i \\times \\log \\text{odds} + \\log(1 + \\exp ^{\\log \\text{odds}})\n",
    "\\end{aligned}$$\n",
    "\n",
    "2. The negative gradient descent w.r.t log odds becomes:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "-g(x_i) & = -\\frac{\\partial}{\\partial \\log \\text{odds}} (-y_i \\times \\log \\text{odds} + \\log(1 + \\exp ^{\\log \\text{odds}}))  = -(-y_i + \\frac{\\exp ^{\\log \\text{odds}}}{1 + \\exp ^{\\log \\text{odds}}}) = y_i - \\hat{p_i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "3. How to find $\\gamma_{jb} = \\underset{\\gamma}{\\argmin} \\sum_{x_i \\in R_{ij}} L(y_i, F_{b-1}(x_i) + \\gamma)$? To find the optimal value, we will take the first order derivative.\n",
    "    * First we use Second Order Taylor Expansion at $F_{b-1}(x_i)$ for the Loss function and Then we take the dirivative w.r.t $\\gamma$:\n",
    "$$\\begin{aligned} \n",
    "L(y_i, F_{b-1}(x_i) + \\gamma) & \\approx L(y_i, F_{b-1}(x_i)) + \\frac{\\partial}{\\partial F}L(y_i, F_{b-1}(x_i)) \\gamma + \\frac{1}{2} \\frac{\\partial^2}{\\partial F^2} L(y_i, F_{b-1}(x_i)) \\gamma^2 \\\\\n",
    "\\rightarrow \\frac{\\partial}{\\partial \\gamma}L(y_i, F_{b-1}(x_i) +  \\gamma) & = \\frac{\\partial}{\\partial F}L(y_i, F_{b-1}(x_i)) + \\frac{\\partial^2}{\\partial F^2} L(y_i, F_{b-1}(x_i)) \\gamma \\\\\n",
    "\\rightarrow \\gamma & = \\frac{-\\frac{\\partial}{\\partial F}L(y_i, F_{b-1}(x_i))}{\\frac{\\partial^2}{\\partial F^2} L(y_i, F_{b-1}(x_i))} = \\frac{y_i - \\hat{p_i}}{\\hat{p_i} \\times (1 - \\hat{p_i})} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "Note:\n",
    "1. Negative gradient pays less attention to outliers than only using residuals and can extend to more differentiable loss functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Classification Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=3,\n",
    "    n_redundant=1,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    random_state=0,\n",
    "    shuffle=False,\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def eval_print(clf, X_train, y_train, X_test, y_test):\n",
    "    print('**************************************')\n",
    "    # Print the training and testing accuracy\n",
    "    print(f'training accuracy: {clf.score(X_train, y_train)}')\n",
    "    print(f'testing accuracy: {clf.score(X_test, y_test)}')\n",
    "    # print training and testing roc_auc_score\n",
    "    print(f'training AUC score: {roc_auc_score(y_train, clf.predict_proba(X_train)[:, 1])}')\n",
    "    print(f'testing AUC score: {roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************\n",
      "training accuracy: 0.9333333333333333\n",
      "testing accuracy: 0.888\n",
      "training AUC score: 0.9816816475139378\n",
      "testing AUC score: 0.9482846902201741\n",
      "**************************************\n",
      "training accuracy: 0.9613333333333334\n",
      "testing accuracy: 0.924\n",
      "training AUC score: 0.9938950676982592\n",
      "testing AUC score: 0.9643177163338454\n",
      "training OOB accuracy: 0.912\n"
     ]
    }
   ],
   "source": [
    "# Bagging classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Just using KNN classifier\n",
    "knn_clf = KNeighborsClassifier().fit(X_train, y_train)\n",
    "eval_print(knn_clf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Use KNN classifier as the base estimator\n",
    "bagging = BaggingClassifier(base_estimator=KNeighborsClassifier(), # The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a DecisionTreeClassifier.\n",
    "                            n_estimators=100, # The number of base estimators in the ensemble, i.e. number of bagging samples B.\n",
    "                            max_samples=0.8,  # The number of samples to draw from X to train each base estimator with replacement by default\n",
    "                            max_features=0.5, # The number of features to draw from X to train each base estimator without replacement by default\n",
    "                            bootstrap=True, # Whether samples are drawn with replacement.\n",
    "                            bootstrap_features=False, # Whether features are drawn with replacement.\n",
    "                            oob_score=True, # Whether to use out-of-bag samples to estimate the generalization error.\n",
    "                            random_state=23) # Controls both the randomness of the bootstrapping of the samples used when building the base estimator\n",
    "clf = bagging.fit(X_train, y_train)\n",
    "eval_print(clf, X_train, y_train, X_test, y_test)\n",
    "# Print the OOB accuracy score\n",
    "print(f'training OOB accuracy: {clf.oob_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************\n",
      "training accuracy: 1.0\n",
      "testing accuracy: 0.908\n",
      "training AUC score: 1.0\n",
      "testing AUC score: 0.9715821812596007\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, # The number of trees in the forest.i.e. number of bagging samples B.\n",
    "                            criterion='gini', # Use 'Gini' as the objective function for the recursive-binary-split\n",
    "                            max_depth=None, # The maximum depth of the tree\n",
    "                            min_samples_split=2, # The minimum number of samples required to split an internal node.\n",
    "                            min_samples_leaf=1, # The minimum number of samples required to be at a leaf node.\n",
    "                            max_features='sqrt', # the number of features to consider at each split: m = sqrt(p).\n",
    "                            bootstrap=True, # Whether bootstrap samples are used when building trees.\n",
    "                            oob_score=True, # Whether to use out-of-bag samples to estimate the generalization score.\n",
    "                            random_state=23, # Controls both the randomness of the bootstrapping of the samples used when building trees \n",
    "                            max_samples=None, # If bootstrap is True, the number of samples to draw from X to train each base estimator. If None (default), then draw X.shape[0] samples.\n",
    ")\n",
    "rf_clf = rf.fit(X_train, y_train)\n",
    "eval_print(rf_clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************\n",
      "training accuracy: 1.0\n",
      "testing accuracy: 0.908\n",
      "training AUC score: 1.0\n",
      "testing AUC score: 0.9715821812596007\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(criterion='gini',\n",
    "                            splitter='best',\n",
    "                            max_depth=1, # The maximum depth of the tree\n",
    "                            min_samples_split=2, # The minimum number of samples required to split an internal node.\n",
    "                            min_samples_leaf=1, # The minimum number of samples required to be at a leaf node.\n",
    "                            max_features=None, # the number of features to consider at each split: m = sqrt(p).\n",
    "                            random_state=None\n",
    ")\n",
    "adbst = AdaBoostClassifier(base_estimator = dt, # The base estimator from which the boosted ensemble is built. If None, then the base estimator is DecisionTreeClassifier initialized with max_depth=1.\n",
    "                          n_estimators=50, # The number of trees in the forest.i.e. number of bagging samples B.\n",
    "                          learning_rate=0.1, # Use 'Gini' as the objective function for the recursive-binary-split\n",
    "                          random_state = 12, # Controls the random seed given at each base_estimator at each boosting iteration.\n",
    ")\n",
    "adbst_clf = rf.fit(X_train, y_train)\n",
    "eval_print(adbst_clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Classification and Regression Trees Course Notes\n",
    "2. Introduction to statistical learning with applications in R\n",
    "3. https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/\n",
    "4. http://www.chengli.io/tutorials/gradient_boosting.pdf\n",
    "5. https://www.youtube.com/@statquest/videos - Gradient Boosting series"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

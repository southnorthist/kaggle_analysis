{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Based Methods\n",
    "\n",
    "*First version on: 2020-11-11  \n",
    " Latest updates on: 2020-11-11*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prediction/Decision Trees\n",
    "What inspires the decision tree methodology?    \n",
    "\n",
    "Predictive models like linear/polynomial regressions are **global models** which means the formula applies to the entire data space. When there are non-linear and complicated interatice effects among features, it is very hard to construct a **global** solution to do prediction.  \n",
    "\n",
    "Some non-parametric smoothers try to fit the data **locally** and then integrate them. While for Prediction/Decision Trees, **sub-division** or **partition** is used to cut the data space into smaller manageable area (i.e. can be modeled using simple average or taking the majority of votes).\n",
    "\n",
    "Note: This information is adapted from reference [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regression Trees\n",
    "### 2.1 What's the prediction mechanism for regression trees\n",
    "There are two steps for the training data:  \n",
    "1. Divide the training predictor space (i.e. $X_1, X_2, \\cdots, X_p$) into $J$ distinct and non-overlapping regions, $R_1, R_2, \\cdots, R_J$.\n",
    "2. For every observation that falls into the region $R_j$, we use the same value for its prediction. The prediction value is the mean of the response values of all the training observation in $R_j$.   \n",
    "\n",
    "How do we construct the regions then?  \n",
    "Let's define the objective function using residual sum of squares (i.e. *RSS*):\n",
    "$$ RSS = \\sum_{j=1}^{J}\\sum_{i \\in R_j} \\left( y_i - \\hat{y}_{R_j} \\right)^{2}$$\n",
    "where $\\hat{y}_{R_j}$ is the mean response for the training observations within the $j^{th}$ region $R_j$.  \n",
    "However, it is computationally infeasible to consider every possible partition of the feature space. So we take a *top-down, greedy* approach which is called as *recursive binary splitting*.\n",
    "> *top-down*: The approach starts with the root of the tree   \n",
    "\n",
    "> *greedy*: We are trying to find the best split at each step rather than looking ahead and picking a split that will lead to a better tree in the future step.   \n",
    "\n",
    "The *recursive binary splitting* algorithm:\n",
    "1. Select the predictor $X_j$ adn cutpoint $s$ such that splitting the predictor space into the regions $R_1(j, s) = \\{X|X_j<s\\}$ and $R_2(j, s) = \\{X|X_j\\ge s\\}$ that leads to the greatest RSS reduction. i.e., select the value of $j$ and $s$ that minimizes:\n",
    "$$ \\sum_{i: x_i \\in R_1((j, s)} \\left( y_i - \\hat{y}_{R_1}\\right) ^2 + \\sum_{i: x_i \\in R_2((j, s)} \\left( y_i - \\hat{y}_{R_2}\\right) ^2$$\n",
    "2. We repeat the process to find the best predictor and best cutpoint that split the above two regions to minimize the RSS within either of the regions. Note that, we will end up split one of the region and the total regions become three.\n",
    "3. We continue the splitting until no region contains more than $M$ number of observations. Typicall, $M = 5$.\n",
    "\n",
    "### 2.2 Why and how do we prune the trees?\n",
    "The *recursive binary splitting* may end up with very good prediction that there is very small RSS (i.e. small bias). In this case, however, the tree may introduce too much complexity to overfit the data and perform very bad on the training data.    \n",
    "\n",
    "This leads to the process of *tree pruning* which tries to the get best *sub-tree* according to some selected hyper parameter $\\alpha$. The methodology is called *cost complexity pruning* or *weakest link pruning*.   \n",
    "\n",
    "For each value of $\\alpha (>0)$ and a full tree $T$, we can find a sub-tree that minimizes the following objective function:\n",
    "$$\\sum_{m = 1}^{|T|}\\sum_{i: x_i \\in R_m} \\left( y_i -\\hat{y}_{R_m}\\right)^2 + \\alpha|T| $$\n",
    "where $|T|$ is the total number of terminal nodes of tree $T$ and $R_m$ is the area corresponding to the $m^{th}$ terminal node.  \n",
    "\n",
    "Here is the complete Regression Tree Building Algorithm:\n",
    "1. Use *recursive binary splitting* to grow a large tree on the trainng data, stopping only when each terminal note has fewer than the minimum number ($M$) of observations.\n",
    "2. Apply *cost complexity pruning* to the large tree in order to obtain a sequence of best subtress, as a function of $\\alpha$.\n",
    "3. Use *K-fold cross-validation* to choose $\\alpha$.\n",
    "4. Return the subtree from Step 2 that corresponds to the chosen value of $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classification Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Classification and Regression Trees Course Notes\n",
    "2. Introduction to statistical learning with applications in R"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

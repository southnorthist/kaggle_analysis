{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "_Summarized by QH_  \n",
    "_First version: 2023-07-08_  \n",
    "_Last updated on : 2023-07-08_  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model hyperparameters vs model parameters?\n",
    "* Model hyperparameters are parameters that control the modeling or learning process. \n",
    "* Model parameters are the parameters that are estimated through training process.\n",
    "* Model hyperparameters will set the direction in the training process and will impact the estimation of model parameters.\n",
    "\n",
    "Think about the knobs on the radios you can use to tune the frequency and volumn - they are the hyperparameters!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Importance\n",
    "\n",
    "Some hyperparameters are more important in terms of determining the model performance than others. \n",
    "\n",
    "For example, random forest classifier:\n",
    "* `n_jobs`: The number of jobs to run in parallel.\n",
    "* `random_state`: Controls both the randomness of the bootstrapping of the samples used when building trees and and the sampling of the features to consider when looking for the best split at each node.\n",
    "* `verbose`: Controls the verbosity when fitting and predicting.\n",
    "\n",
    "do not impact the model performance compared to the following:\n",
    "* `n_estimators`: The number of trees to build.\n",
    "* `max_features`: The number of features to consider when looking for best splits.\n",
    "* `max_depth`: The maximum depth of the tree.\n",
    "* `min_sample_leaf`: The minimum number of samples required to be at a leaf node."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to choose hyperparameters?\n",
    "The process is called the hyperparameter tuning - we want to tune as best as we can on the hyperparameters that has the lowest predicting error.\n",
    "\n",
    "### Constraints among hyperparameters\n",
    "Be careful on the potential conflict of hyperparameters. For example, `LogisticRegression()` has conflicting hyperparameter options of `solver` and `penalty`.\n",
    "### Avoiding \"Silly\" choices\n",
    "Certain values that will definitely not contribute to decent model performance, then avoid them.\n",
    "* Random Forest\n",
    "    * low number of trees (`n_estimators`)\n",
    "* K-Nearest Neighbor\n",
    "    * <= 2 Neighbors (`n_neighbors`)\n",
    "* Increasing a hyperparameter by a very small amount compared with its range.\n",
    "\n",
    "### Grid Search\n",
    "For each of the hyperparameters you want to tune, list all chosen values and test each combinations and find the combination with the best model performance. For example, Gradient boosting, we want to tune the following hyperparameters:\n",
    "* `learn_rate`: [0.001, 0.01, 0.1, 0.2]\n",
    "* `max_depth`: [4, 6, 8, 10, 12, 15, 20, 25, 30]\n",
    "* `subsample`: [0.4, 0.6, 0.7, 0.8, 0.9]\n",
    "* `max_features`: ['auto', 'sqrt']\n",
    "\n",
    "We will test all $4 \\times 9 \\times 5 \\times 2 = 360$ combinations. If we want to use 10 fold cross-validation, then we will in total make $360 \\times 10 = 3600$ models.\n",
    "\n",
    "* Advantanges of Grid Search\n",
    "    * You are guaranteed to the find the best results in this grid - since you have performed an exhaustive search.\n",
    "* Disadvantages of Grid Search\n",
    "    * It is computationally expensive. The cost increases exponentially when adding more hyperparameters and testing more hyperparameter values.\n",
    "    * It is uninformed. Running models are independent - previous model do not inform the next choice.\n",
    "\n",
    "You can do mannually as looping through each combination as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python packages\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the list of values to test\n",
    "learn_rate_list = [0.001, 0.01, 0.1, 0.2, 0.3, 0.5]\n",
    "max_depth_list = [4,6,8, 10, 12, 15, 20, 25, 30]\n",
    "subsample_list = [0.7]\n",
    "max_features_list = ['sqrt']\n",
    "\n",
    "def gbm_grid_search(learn_rate, max_depth,subsample,max_features):\n",
    "    model = GradientBoostingClassifier(\n",
    "    learning_rate=learn_rate,\n",
    "    max_depth=max_depth,\n",
    "    subsample=subsample,\n",
    "    max_features=max_features)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    return([learn_rate, max_depth, subsample, max_features, roc_auc_score(y_test, predictions)])\n",
    "\n",
    "results_list = []\n",
    "for learn_rate in learn_rate_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        for subsample in subsample_list:\n",
    "            for max_features in max_features_list:\n",
    "                results_list.append(gbm_grid_search(learn_rate,max_depth, subsample,max_features))\n",
    "results_df = pd.DataFrame(results_list, columns=['learning_rate', 'max_depth', 'subsample', 'max_features','auc'])\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the performance by hyperparameter values\n",
    "sns.scatterplot(data=results_df, x='max_depth', y='auc')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can use `GridSearchCV` in `scikit-learn` as follows:\n",
    "\n",
    "```\n",
    "sklearn.model_selection.GridSearchCV(\n",
    "    estimator,\n",
    "    param_grid, \n",
    "    scoring=None, \n",
    "    n_jobs=None, # Number of jobs to run in parallel.\n",
    "    refit=True, # Refit an estimator using the best found parameters on the whole dataset.\n",
    "    cv=10,\n",
    "    verbose=0, # Controls the verbosity: the higher, the more messages. \n",
    "    pre_dispatch='2*n_jobs', # Controls the number of jobs that get dispatched during parallel execution.\n",
    "    error_score='raise', # Value to assign to the score if an error occurs in estimator fitting.\n",
    "    return_train_score=True # Computing training scores is used to get insights on how different parameter settings impact the overfitting/underfitting trade-off.\n",
    ")\n",
    "```\n",
    "\n",
    "1. Define the method and only one method (e.g. Random Forest) we use for modeling - `estimator`\n",
    "2. Choose and hyperparameters and Define the hyperparameter value grid as a dictionary - `param_grid`\n",
    "3. Set a cross-validation scheme as how many folds to be performed - `cv`\n",
    "4. Choose/Define a scoring function to evelation model performance - `scoring`\n",
    "5. Choose more information for the Grid Search Process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3, 0.5],\n",
    "    'max_depth': [4,6,8, 10, 12, 15, 20, 25, 30],\n",
    "    'subsample': [0.7],\n",
    "    'max_features': ['sqrt']\n",
    "}\n",
    "\n",
    "grid_gb_clf = GridSearchCV(\n",
    "    GradientBoostingClassifier(),\n",
    "    param_grid=param_grid, \n",
    "    scoring='roc_auc', \n",
    "    n_jobs=4, # Number of jobs to run in parallel.\n",
    "    refit=True, # Refit an estimator using the best found parameters on the whole dataset.\n",
    "    cv=5,\n",
    "    verbose=0, # Controls the verbosity: the higher, the more messages. \n",
    "    pre_dispatch='2*n_jobs', # Controls the number of jobs that get dispatched during parallel execution.\n",
    "    error_score='raise', # Value to assign to the score if an error occurs in estimator fitting.\n",
    "    return_train_score=True # Computing training scores is used to get insights on how different parameter settings impact the overfitting/underfitting trade-off.\n",
    ")\n",
    "# Fit the training data\n",
    "grid_gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Get the training results\n",
    "grid_gb_clf.cv_results_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Search\n",
    "Similar to Grid Search, we define the hypermeter value grid. The difference is that we don't search the entire grid but randomly choose a range for each parameter to test.\n",
    "\n",
    "Bengio & Bergstra (2012): This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. \n",
    "There are two main reasons:\n",
    "1. Not all hyperparameters are of equal importantance to contribute to model performance\n",
    "2. As long as we try enough time, we can achieve a high probabilty of getting a decent good model - magic of probability!\n",
    "\n",
    "Imagine for 100 combinations of hyperparameters, there are 5 of them that can generage best model performance.\n",
    "* If we try 1 time, the probability of missing the best selections is $1-0.05 = 0.95$\n",
    "* If we try 2 times, the probability of missing the best selections is $(1-0.05) \\times (1-0.05) = 0.95^2$\n",
    "* ...\n",
    "* If we try n times, the probability of missing the best selections is $(1-0.05) \\times \\cdots \\times (1-0.05) = 0.95^n$ which means the if we try n times, the probability of hitting the best selections is $1 - 0.95^n$ - We only need 59 times of trial and the probability of selecting the best 5 is greater than 95%!\n",
    "\n",
    "You can use `RandomizedSearchCV` in `scikit-learn` as follows:\n",
    "\n",
    "```\n",
    "sklearn.model_selection.RandomizedSearchCV(\n",
    "    estimator,\n",
    "    param_distributions,\n",
    "    n_iter, \n",
    "    scoring=None, \n",
    "    n_jobs=None, # Number of jobs to run in parallel.\n",
    "    refit=True, # Refit an estimator using the best found parameters on the whole dataset.\n",
    "    cv=10,\n",
    "    verbose=0, # Controls the verbosity: the higher, the more messages. \n",
    "    pre_dispatch='2*n_jobs', # Controls the number of jobs that get dispatched during parallel execution.\n",
    "    error_score='raise', # Value to assign to the score if an error occurs in estimator fitting.\n",
    "    return_train_score=True # Computing training scores is used to get insights on how different parameter settings impact the overfitting/underfitting trade-off.\n",
    ")\n",
    "```\n",
    "\n",
    "1. Define the method and only one method (e.g. Random Forest) we use for modeling - `estimator`\n",
    "2. Choose and hyperparameters and Define the hyperparameter value distribution as a dictionary - `param_distributions`\n",
    "3. Decide how many combinations you want to try - `n_iter` \n",
    "4. Set a cross-validation scheme as how many folds to be performed - `cv`\n",
    "5. Choose/Define a scoring function to evelation model performance - `scoring`\n",
    "6. Choose more information for the Grid Search Process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. [Hyperparameter Tuning Course on Datacamp](https://app.datacamp.com/learn/courses/hyperparameter-tuning-in-python)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

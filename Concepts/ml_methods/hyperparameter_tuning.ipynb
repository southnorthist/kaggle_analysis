{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "_Summarized by QH_  \n",
    "_First version: 2023-07-08_  \n",
    "_Last updated on : 2023-07-08_  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model hyperparameters vs model parameters?\n",
    "* Model hyperparameters are parameters that control the modeling or learning process. \n",
    "* Model parameters are the parameters that are estimated through training process.\n",
    "* Model hyperparameters will set the direction in the training process and will impact the estimation of model parameters.\n",
    "\n",
    "Think about the knobs on the radios you can use to tune the frequency and volumn - they are the hyperparameters!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Importance\n",
    "\n",
    "Some hyperparameters are more important in terms of determining the model performance than others. \n",
    "\n",
    "For example, random forest classifier:\n",
    "* `n_jobs`: The number of jobs to run in parallel.\n",
    "* `random_state`: Controls both the randomness of the bootstrapping of the samples used when building trees and and the sampling of the features to consider when looking for the best split at each node.\n",
    "* `verbose`: Controls the verbosity when fitting and predicting.\n",
    "\n",
    "do not impact the model performance compared to the following:\n",
    "* `n_estimators`: The number of trees to build.\n",
    "* `max_features`: The number of features to consider when looking for best splits.\n",
    "* `max_depth`: The maximum depth of the tree.\n",
    "* `min_sample_leaf`: The minimum number of samples required to be at a leaf node."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to choose hyperparameters?\n",
    "The process is called the hyperparameter tuning - we want to tune as best as we can on the hyperparameters that has the lowest predicting error.\n",
    "\n",
    "### Constraints among hyperparameters\n",
    "Be careful on the potential conflict of hyperparameters. For example, `LogisticRegression()` has conflicting hyperparameter options of `solver` and `penalty`.\n",
    "### Avoiding \"Silly\" choices\n",
    "Certain values that will definitely not contribute to decent model performance, then avoid them.\n",
    "* Random Forest\n",
    "    * low number of trees (`n_estimators`)\n",
    "* K-Nearest Neighbor\n",
    "    * <= 2 Neighbors (`n_neighbors`)\n",
    "* Increasing a hyperparameter by a very small amount compared with its range.\n",
    "\n",
    "### Grid Search\n",
    "For each of the hyperparameters you want to tune, list all chosen values and test each combinations and find the combination with the best model performance. For example, Gradient boosting, we want to tune the following hyperparameters:\n",
    "* `learn_rate`: [0.001, 0.01, 0.1, 0.2]\n",
    "* `max_depth`: [4, 6, 8, 10, 12, 15, 20, 25, 30]\n",
    "* `subsample`: [0.4, 0.6, 0.7, 0.8, 0.9]\n",
    "* `max_features`: ['auto', 'sqrt']\n",
    "\n",
    "We will test all $4 \\times 9 \\times 5 \\times 2 = 360$ combinations. If we want to use 10 fold cross-validation, then we will in total make $360 \\times 10 = 3600$ models.\n",
    "\n",
    "* Advantanges of Grid Search\n",
    "    * You are guaranteed to the find the best results in this grid - since you have performed an exhaustive search.\n",
    "* Disadvantages of Grid Search\n",
    "    * It is computationally expensive. The cost increases exponentially when adding more hyperparameters and testing more hyperparameter values.\n",
    "    * It is uninformed. Running models are independent - previous model do not inform the next choice.\n",
    "\n",
    "You can do mannually as looping through each combination as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python packages\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the list of values to test\n",
    "learn_rate_list = [0.001, 0.01, 0.1, 0.2]\n",
    "max_depth_list = [4,6,8, 10, 12, 15, 20, 25, 30]\n",
    "subsample_list = [0.4,0.6, 0.7, 0.8, 0.9]\n",
    "max_features_list = ['log2', 'sqrt']\n",
    "\n",
    "def gbm_grid_search(learn_rate, max_depth,subsample,max_features):\n",
    "    model = GradientBoostingClassifier(\n",
    "    learning_rate=learn_rate,\n",
    "    max_depth=max_depth,\n",
    "    subsample=subsample,\n",
    "    max_features=max_features)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    return([learn_rate, max_depth, accuracy_score(y_test, predictions)])\n",
    "\n",
    "results_list = []\n",
    "for learn_rate in learn_rate_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        for subsample in subsample_list:\n",
    "            for max_features in max_features_list:\n",
    "                results_list.append(gbm_grid_search(learn_rate,max_depth, subsample,max_features))\n",
    "results_df = pd.DataFrame(results_list, columns=['learning_rate', 'max_depth', 'subsample', 'max_features','accuracy'])\n",
    "print(results_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can use `GridSearchCV` in `scikit-learn` as follows:\n",
    "\n",
    "```\n",
    "sklearn.model_selection.GridSearchCV(\n",
    "                                    estimator,\n",
    "                                    param_grid, \n",
    "                                    scoring=None, \n",
    "                                    fit_params=None,\n",
    "                                    n_jobs=None, \n",
    "                                    refit=True, \n",
    "                                    cv='warn',\n",
    "                                    verbose=0, \n",
    "                                    pre_dispatch='2*n_jobs',\n",
    "                                    error_score='raise-deprecating',\n",
    "                                    return_train_score='warn'\n",
    "                                    )\n",
    "```\n",
    "\n",
    "1. Define the method (e.g. Random Forest) we use for modeling - `estimator`\n",
    "2. Define the hyperparameter grid - `param_grid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. [Hyperparameter Tuning Course on Datacamp](https://app.datacamp.com/learn/courses/hyperparameter-tuning-in-python)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

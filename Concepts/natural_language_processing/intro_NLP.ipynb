{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "\n",
    "_Summarized by QH_  \n",
    "_First version: 2023-07-16_  \n",
    "_Last updated on : 2023-07-23_  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is NLP?\n",
    "It is a field of using statistics and computers to understand languages. It can help to identify topic, classify text.\n",
    "\n",
    "We can use NLP in the following applications:\n",
    "* Chatbots\n",
    "* Translation\n",
    "* Sentiment analysis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prepocessing\n",
    "#### Tokenization\n",
    "Tokens are smaller chunks of text from a string or document which can help us understand better of the text (e.g., excluding unwanted or uninformative chunks).\n",
    "\n",
    "The process of breaking into tokens is called _Tokenization_.\n",
    "We can use regular expressions help with this task, e.g.:\n",
    "* Breaking out words or sentences\n",
    "* Separating punctuaion\n",
    "* Separating all hashtags in a tweet.\n",
    "#### Stemming and Lemmatization\n",
    "Lemmatization: determining the root word.\n",
    "Stemming: simpler version of lemmatization - stripping suffixes from the end of the word.\n",
    "\n",
    "#### Sentence segmentation\n",
    "Sentence segmentation: breaking up a text into individual sentences, using cues like perios or exclamation points.\n",
    "\n",
    "#### Stop word removal\n",
    "Stop words are words like “and”, “the”, “him”, which are presumed to be uninformative in representing the content of a text, and which may be removed to avoid them being construed as signal for prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several python packages that helps with tokenization.\n",
    "* `re`\n",
    "* `tokenize` from `nltk`\n",
    "    * `word_tokenize`: tokenize a document into words.\n",
    "    * `sent_tokenize`: tokenize a document into sentences.\n",
    "    * `regexp_tokenize`: tokenize a string or document based on a regular expression pattern\n",
    "    * `TweetTokenizer`: special class just for tweet tokenization, allowing you to separate hashtags, mentions and lots of exclamation points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "\n",
    "#### Bag-of-words (BOW)\n",
    "When doing text analysis, strings need to be transformed into numeric values to be fed to the algorithms. \"Bag-of-words\" or \"Bag of n-grams\" representation is a strategy used to transform a text document into numeric features - Builds a vocabulary of the words and a measure of presence:\n",
    "\n",
    "The following is the process:\n",
    "* __tokenizing__ strings and giving an integer id for each possible token (using white-spaces and puncuations as token seperators).\n",
    "* __counting__ the occurrences of tokens in each document\n",
    "* __normalizing__ and weighting with diminishing importance tokens that occur in the majority of samples/documents.\n",
    "\n",
    "As a result of this process (we call it __vectorization__):\n",
    "* each __individual token occurrence frequency__ (normalized or not) is treated as a __feature__\n",
    "* the vector of all the token frequencies for a given __document__ is considered a multivariate __sample__.\n",
    "\n",
    "##### Sparsity\n",
    "As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have feature values that are zeros (typically more than 99% of them).\n",
    "\n",
    "In order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector, implementations will typically use a sparse representation \n",
    "##### n-grams\n",
    "Even thought the number of each words are exactly the same, the meaning is different - which means context matters:\n",
    "* \"I am happy, not sad.\"\n",
    "* \"I am sad, not happy.\"\n",
    "\n",
    "1. Unigrams: single tokes\n",
    "2. Bigrams: pairs of tokens\n",
    "3. Trigrams: triple of tokens\n",
    "4. n-grams: sequence of n-tokens\n",
    "\n",
    "\"The weather today is wonderful.\"\n",
    "* Unigrams : { The, weather, today, is, wonderful }\n",
    "* Bigrams: {The weather, weather today, today is, is wonderful}\n",
    "* Trigrams: {The weather today, weather today is, today is wonderful}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<5x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 24 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 1, 0, 0, 1, 0, 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['and', 'document', 'first', 'is', 'last', 'one', 'second', 'the',\n",
       "       'third', 'this'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bi-grams:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<5x24 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 44 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['and', 'and the', 'document', 'first', 'first document', 'is',\n",
       "       'is the', 'is this', 'last', 'last document', 'one', 'second',\n",
       "       'second document', 'second second', 'the', 'the first', 'the last',\n",
       "       'the second', 'the third', 'third', 'third one', 'this', 'this is',\n",
       "       'this the'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# initialize the vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "# bigrams\n",
    "bigram_vect = CountVectorizer(ngram_range=(1,2))\n",
    "# create the corpus\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'This is the last document.'\n",
    "]\n",
    "print(\"Unigram:\\n\")\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "display(X)\n",
    "# Get a glimpse of X\n",
    "display(X.toarray())\n",
    "# get the token name\n",
    "token_name = vectorizer.get_feature_names_out()\n",
    "display(token_name)\n",
    "\n",
    "print('\\nBi-grams:\\n')\n",
    "Y = bigram_vect.fit_transform(corpus)\n",
    "display(Y)\n",
    "display(bigram_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "In a large text corpus, some words will be very commonly used (e.g. \"the\", \"a\", \"is\", etc.) hence carrying very little meaningful information. As a result, if we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more informative terms. So we need to do some transformation to defactor the impact, and Tf-idf is a technique.\n",
    "* Tf: __term_frequency__\n",
    "* idf: __inverse document-frequency__\n",
    "\n",
    "$$\n",
    "\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t)\n",
    "$$\n",
    "where \n",
    "$\\text{tf}(t, d)$ is the number of term $t$ appear in document $d$ and\n",
    "$$\\begin{aligned}\n",
    "\\text{idf}(t) &= \\log \\bigg( \\frac{1 + n}{1 + \\text{df}(t)} \\bigg) + 1 \\ (smoothed) \\\\\n",
    "\\text{idf}(t) &= \\log \\bigg( \\frac{n}{\\text{df}(t)} \\bigg) + 1 \\ (unsmoothed)\n",
    "\\end{aligned}$$\n",
    "$n$ is the total number of documents in the document set and $\\text{df}(t)$ is the number of documents in the document set that contain term $t$.\n",
    "\n",
    "The resulting tf-idf vectors are then normalized for each document by Euclidean norm:\n",
    "$$v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 + v{_2}^2 + \\dots + v{_n}^2}}$$\n",
    "\n",
    "The following is the example using `TfidfTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81940995, 0.        , 0.57320793],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.47330339, 0.88089948, 0.        ],\n",
       "       [0.58149261, 0.        , 0.81355169]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "# the following is the term frequency matrix(row - document, column - term)\n",
    "counts = np.array(\n",
    "         [[3, 0, 1],          \n",
    "          [2, 0, 0],\n",
    "          [3, 0, 0],\n",
    "          [4, 0, 0],\n",
    "          [3, 2, 0],\n",
    "          [3, 0, 2]])\n",
    "# using tfidf transformer on the counts\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81940995, 0.        , 0.57320793],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.47330339, 0.88089948, 0.        ],\n",
       "       [0.58149261, 0.        , 0.81355169]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now - calculate manually\n",
    "n = counts.shape[0]\n",
    "df = np.sum(counts > 0, axis=0)\n",
    "idf = np.log(n / df) + 1\n",
    "# elementwise multiplication\n",
    "tf_idf = counts * idf \n",
    "# normalization by document\n",
    "norm_tf_idf = tf_idf / np.sqrt(np.sum(np.power(tf_idf,2), axis=1))[:, None]\n",
    "norm_tf_idf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

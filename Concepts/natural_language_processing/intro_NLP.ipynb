{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "\n",
    "_Summarized by QH_  \n",
    "_First version: 2023-07-16_  \n",
    "_Last updated on : 2023-07-23_  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is NLP?\n",
    "It is a field of using statistics and computers to understand languages. It can help to identify topic, classify text.\n",
    "\n",
    "We can use NLP in the following applications:\n",
    "* Chatbots\n",
    "* Translation\n",
    "* Sentiment analysis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prepocessing\n",
    "#### Tokenization\n",
    "Tokens are smaller chunks of text from a string or document which can help us understand better of the text (e.g., excluding unwanted or uninformative chunks).\n",
    "\n",
    "The process of breaking into tokens is called _Tokenization_.\n",
    "We can use regular expressions help with this task, e.g.:\n",
    "* Breaking out words or sentences\n",
    "* Separating punctuaion\n",
    "* Separating all hashtags in a tweet.\n",
    "#### Stemming and Lemmatization\n",
    "Lemmatization: determining the root word.\n",
    "Stemming: simpler version of lemmatization - stripping suffixes from the end of the word.\n",
    "\n",
    "#### Sentence segmentation\n",
    "Sentence segmentation: breaking up a text into individual sentences, using cues like perios or exclamation points.\n",
    "\n",
    "#### Stop word removal\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several python packages that helps with tokenization.\n",
    "* `re`\n",
    "* `tokenize` from `nltk`\n",
    "    * `word_tokenize`: tokenize a document into words.\n",
    "    * `sent_tokenize`: tokenize a document into sentences.\n",
    "    * `regexp_tokenize`: tokenize a string or document based on a regular expression pattern\n",
    "    * `TweetTokenizer`: special class just for tweet tokenization, allowing you to separate hashtags, mentions and lots of exclamation points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "\n",
    "#### Bag-of-words (BOW)\n",
    "tba.\n",
    "#### Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "tba.\n",
    "#### Word2Vec\n",
    "tba.\n",
    "#### GLoVE\n",
    "tba."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

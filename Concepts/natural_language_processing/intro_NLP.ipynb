{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "\n",
    "_Summarized by QH_  \n",
    "_First version: 2023-07-16_  \n",
    "_Last updated on : 2023-07-23_  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is NLP?\n",
    "It is a field of using statistics and computers to understand languages. It can help to identify topic, classify text.\n",
    "\n",
    "We can use NLP in the following applications:\n",
    "* Chatbots\n",
    "* Translation\n",
    "* Sentiment analysis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prepocessing\n",
    "#### Tokenization\n",
    "Tokens are smaller chunks of text from a string or document which can help us understand better of the text (e.g., excluding unwanted or uninformative chunks).\n",
    "\n",
    "The process of breaking into tokens is called _Tokenization_.\n",
    "We can use regular expressions help with this task, e.g.:\n",
    "* Breaking out words or sentences\n",
    "* Separating punctuaion\n",
    "* Separating all hashtags in a tweet.\n",
    "#### Stemming and Lemmatization\n",
    "Lemmatization: determining the root word.\n",
    "Stemming: simpler version of lemmatization - stripping suffixes from the end of the word.\n",
    "\n",
    "#### Sentence segmentation\n",
    "Sentence segmentation: breaking up a text into individual sentences, using cues like perios or exclamation points.\n",
    "\n",
    "#### Stop word removal\n",
    "Stop words are words like “and”, “the”, “him”, which are presumed to be uninformative in representing the content of a text, and which may be removed to avoid them being construed as signal for prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several python packages that helps with tokenization.\n",
    "* `re`\n",
    "* `tokenize` from `nltk`\n",
    "    * `word_tokenize`: tokenize a document into words.\n",
    "    * `sent_tokenize`: tokenize a document into sentences.\n",
    "    * `regexp_tokenize`: tokenize a string or document based on a regular expression pattern\n",
    "    * `TweetTokenizer`: special class just for tweet tokenization, allowing you to separate hashtags, mentions and lots of exclamation points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "\n",
    "#### Bag-of-words (BOW)\n",
    "When doing text analysis, strings need to be transformed into numeric values to be fed to the algorithms. \"Bag-of-words\" or \"Bag of n-grams\" representation is a strategy used to transform a text document into numeric features - Builds a vocabulary of the words and a measure of presence:\n",
    "\n",
    "The following is the process:\n",
    "* __tokenizing__ strings and giving an integer id for each possible token (using white-spaces and puncuations as token seperators).\n",
    "* __counting__ the occurrences of tokens in each document\n",
    "* __normalizing__ and weighting with diminishing importance tokens that occur in the majority of samples/documents.\n",
    "\n",
    "As a result of this process (we call it __vectorization__):\n",
    "* each __individual token occurrence frequency__ (normalized or not) is treated as a __feature__\n",
    "* the vector of all the token frequencies for a given __document__ is considered a multivariate __sample__.\n",
    "\n",
    "##### Sparsity\n",
    "As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have feature values that are zeros (typically more than 99% of them).\n",
    "\n",
    "In order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector, implementations will typically use a sparse representation \n",
    "##### n-grams\n",
    "Even thought the number of each words are exactly the same, the meaning is different - which means context matters:\n",
    "* \"I am happy, not sad.\"\n",
    "* \"I am sad, not happy.\"\n",
    "\n",
    "1. Unigrams: single tokes\n",
    "2. Bigrams: pairs of tokens\n",
    "3. Trigrams: triple of tokens\n",
    "4. n-grams: sequence of n-tokens\n",
    "\n",
    "\"The weather today is wonderful.\"\n",
    "* Unigrams : { The, weather, today, is, wonderful }\n",
    "* Bigrams: {The weather, weather today, today is, is wonderful}\n",
    "* Trigrams: {The weather today, weather today is, today is wonderful}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<5x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 24 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 1, 0, 0, 1, 0, 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['and', 'document', 'first', 'is', 'last', 'one', 'second', 'the',\n",
       "       'third', 'this'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bi-grams:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<5x24 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 44 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['and', 'and the', 'document', 'first', 'first document', 'is',\n",
       "       'is the', 'is this', 'last', 'last document', 'one', 'second',\n",
       "       'second document', 'second second', 'the', 'the first', 'the last',\n",
       "       'the second', 'the third', 'third', 'third one', 'this', 'this is',\n",
       "       'this the'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# initialize the vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "# bigrams\n",
    "bigram_vect = CountVectorizer(ngram_range=(1,2))\n",
    "# create the corpus\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'This is the last document.'\n",
    "]\n",
    "print(\"Unigram:\\n\")\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "display(X)\n",
    "# Get a glimpse of X\n",
    "display(X.toarray())\n",
    "# get the token name\n",
    "token_name = vectorizer.get_feature_names_out()\n",
    "display(token_name)\n",
    "\n",
    "print('\\nBi-grams:\\n')\n",
    "Y = bigram_vect.fit_transform(corpus)\n",
    "display(Y)\n",
    "display(bigram_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "In a large text corpus, some words will be very commonly used (e.g. \"the\", \"a\", \"is\", etc.) hence carrying very little meaningful information. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms. So we need to do some transformation, Tf-idf is a technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec\n",
    "tba.\n",
    "#### GLoVE\n",
    "tba."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
